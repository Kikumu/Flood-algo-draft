{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from random import seed\n",
    "from sklearn import preprocessing\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_lstm(nn.Module):\n",
    "    def __init__(self, lr, input_channels = 1, batch_size = 1, lstm_hidden_size = 512, lstm_layer_size = 2):\n",
    "        super(conv_lstm, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.input_channels = input_channels\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.height = height\n",
    "        #self.width = width\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_layer_size = lstm_layer_size\n",
    "        self.conv_layer = nn.conv2d(in_channels = self.input_channels, out_channels = 10,\n",
    "                                   kernel_size = 3, stride = 1)\n",
    "        conv_layer = 1.0/np.sqrt(self.conv_layer.weight.data.size[0])\n",
    "        self.conv_layer.weight.data.uniform(-conv_layer, conv_layer)\n",
    "        self.conv_layer.bias.data.unifrom(-conv_layer, conv_layer)\n",
    "        self.conv_layer_batchNorm = nn.BatchNorm(10)\n",
    "        self.pool = nn.MaxPool2d((2,4), stride=1)#experiment with maxpool/avgpool\n",
    "        #second querry - should i maxpool/avgpool then send to lstm layer or leave it as it is? Refer to paper\n",
    "        \n",
    "        #heavily dependant on convlayer out shape\n",
    "        self.lstm_layer = nn.LSTM(input_size = 50,#this needs to be adjusted\n",
    "                                 hidden_size = self.lstm_hidden_size,\n",
    "                                 num_layers  = self.lstm_layer_size,\n",
    "                                 batch_first = True,\n",
    "                                 dropout = 0.1)\n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        x = self.conv_layer(frames)\n",
    "        x = self.conv_layer_batchNorm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        print(\"forward shape: \", x.shape)\n",
    "        batch_size = x.size(0)\n",
    "        hidden_state = torch.zeros(self.lstm_layer_size, \n",
    "                                  batch_size,\n",
    "                                  self.lstm_hidden_size).to(self.device)\n",
    "        cell_state = torch.zeros(self.lstm_layer_size,\n",
    "                                batch_size,\n",
    "                                self.lstm_hidden_size).to(self.device)\n",
    "        hidden_lstm_layer = (hidden_state, cell_state)\n",
    "        out, (hn,cn) = self.lstm_layer(x,(hidden_state, cell_state))\n",
    "        \n",
    "        #might need fc layer will check with paper\n",
    "        return out, hn\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
